{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class for plot the diagram\n",
    "\n",
    "class plot_error_surfaces(object):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n",
    "        W = np.linspace(-w_range, w_range, n_samples)\n",
    "        B = np.linspace(-b_range, b_range, n_samples)\n",
    "        w, b = np.meshgrid(W, B)    \n",
    "        Z = np.zeros((30, 30))\n",
    "        count1 = 0\n",
    "        self.y = Y.numpy()\n",
    "        self.x = X.numpy()\n",
    "        for w1, b1 in zip(w, b):\n",
    "            count2 = 0\n",
    "            for w2, b2 in zip(w1, b1):\n",
    "                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n",
    "                count2 += 1\n",
    "            count1 += 1\n",
    "        self.Z = Z\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        self.LOSS = []\n",
    "        self.n = 0\n",
    "        if go == True:\n",
    "            plt.figure()\n",
    "            plt.figure(figsize = (7.5, 5))\n",
    "            plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')\n",
    "            plt.title('Loss Surface')\n",
    "            plt.xlabel('w')\n",
    "            plt.ylabel('b')\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            plt.title('Loss Surface Contour')\n",
    "            plt.xlabel('w')\n",
    "            plt.ylabel('b')\n",
    "            plt.contour(self.w, self.b, self.Z)\n",
    "            plt.show()\n",
    "    \n",
    "    # Setter\n",
    "    def set_para_loss(self, W, B, loss):\n",
    "        self.n = self.n + 1\n",
    "        self.W.append(W)\n",
    "        self.B.append(B)\n",
    "        self.LOSS.append(loss)\n",
    "    \n",
    "    # Plot diagram\n",
    "    def final_plot(self): \n",
    "        ax = plt.axes(projection = '3d')\n",
    "        ax.plot_wireframe(self.w, self.b, self.Z)\n",
    "        ax.scatter(self.W, self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n",
    "        plt.figure()\n",
    "        plt.contour(self.w, self.b, self.Z)\n",
    "        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('b')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot diagram\n",
    "    def plot_ps(self):\n",
    "        plt.subplot(121)\n",
    "        plt.ylim\n",
    "        plt.plot(self.x, self.y, 'ro', label = \"training points\")\n",
    "        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.ylim((-10, 15))\n",
    "        plt.title('Data Space Iteration: ' + str(self.n))\n",
    "        plt.subplot(122)\n",
    "        plt.contour(self.w, self.b, self.Z)\n",
    "        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n",
    "        plt.title('Loss Surface Contour Iteration' + str(self.n))\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('b')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Setup the actual data and simulated data\n",
    "\n",
    "X = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "f = 1 * X - 1\n",
    "Y = f + 0.1 * torch.randn(X.size())\n",
    "\n",
    "plt.plot(X.numpy(), Y.numpy(), 'rx', label = 'y')\n",
    "plt.plot(X.numpy(), f.numpy(), label = 'f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward function\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "# Define the MSE Loss function\n",
    "\n",
    "def criterion(yhat, y):\n",
    "    return torch.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot_error_surfaces for viewing the data\n",
    "\n",
    "get_surface = plot_error_surfaces(15, 13, X, Y, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters w, b for y = wx + b\n",
    "\n",
    "w = torch.tensor(-15.0, requires_grad = True)\n",
    "b = torch.tensor(-10.0, requires_grad = True)\n",
    "\n",
    "# Define learning rate and create an empty list for containing the loss for each iteration.\n",
    "\n",
    "lr = 0.1\n",
    "LOSS_BGD = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for training the model\n",
    "\n",
    "def train_model(iter):\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(iter):\n",
    "        \n",
    "        # make a prediction\n",
    "        Yhat = forward(X)\n",
    "        \n",
    "        # calculate the loss \n",
    "        loss = criterion(Yhat, Y)\n",
    "\n",
    "        # Section for plotting\n",
    "        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
    "        get_surface.plot_ps()\n",
    "            \n",
    "        # store the loss in the list LOSS_BGD\n",
    "        LOSS_BGD.append(loss)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters slope and bias\n",
    "        w.data = w.data - lr * w.grad.data\n",
    "        b.data = b.data - lr * b.grad.data\n",
    "        \n",
    "        # zero the gradients before running the backward pass\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with 10 iterations\n",
    "\n",
    "train_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot_error_surfaces for viewing the data\n",
    "\n",
    "get_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for training the model\n",
    "\n",
    "LOSS_SGD = []\n",
    "w = torch.tensor(-15.0, requires_grad = True)\n",
    "b = torch.tensor(-10.0, requires_grad = True)\n",
    "\n",
    "def train_model_SGD(iter):\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(iter):\n",
    "        \n",
    "        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n",
    "        Yhat = forward(X)\n",
    "\n",
    "        # store the loss \n",
    "        LOSS_SGD.append(criterion(Yhat, Y).tolist())\n",
    "        \n",
    "        for x, y in zip(X, Y):\n",
    "            \n",
    "            # make a pridiction\n",
    "            yhat = forward(x)\n",
    "        \n",
    "            # calculate the loss \n",
    "            loss = criterion(yhat, y)\n",
    "\n",
    "            # Section for plotting\n",
    "            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
    "        \n",
    "            # backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "            loss.backward()\n",
    "        \n",
    "            # update parameters slope and bias\n",
    "            w.data = w.data - lr * w.grad.data\n",
    "            b.data = b.data - lr * b.grad.data\n",
    "\n",
    "            # zero the gradients before running the backward pass\n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "            \n",
    "        #plot surface and data space after each epoch    \n",
    "        get_surface.plot_ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model_SGD(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the LOSS_BGD and LOSS_SGD\n",
    "LOSS_BGD= [ loss.detach().numpy() for loss in LOSS_BGD]\n",
    "plt.plot(LOSS_BGD,label = \"Batch Gradient Descent\")\n",
    "plt.plot(LOSS_SGD,label = \"Stochastic Gradient Descent\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cost/ total loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset Class\n",
    "\n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "        self.y = 1 * self.x - 1\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    # Getter\n",
    "    def __getitem__(self,index):    \n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    # Return the length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and check the length\n",
    "\n",
    "dataset = Data()\n",
    "print(\"The length of dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first point\n",
    "\n",
    "x, y = dataset[0]\n",
    "print(\"(\", x, \", \", y, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 3 point\n",
    "\n",
    "x, y = dataset[0:3]\n",
    "print(\"The first 3 x: \", x)\n",
    "print(\"The first 3 y: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot_error_surfaces for viewing the data\n",
    "\n",
    "get_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n",
    "\n",
    "# Create DataLoader\n",
    "\n",
    "trainloader = DataLoader(dataset = dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for training the model\n",
    "\n",
    "w = torch.tensor(-15.0,requires_grad=True)\n",
    "b = torch.tensor(-10.0,requires_grad=True)\n",
    "LOSS_Loader = []\n",
    "\n",
    "def train_model_DataLoader(epochs):\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n",
    "        Yhat = forward(X)\n",
    "        \n",
    "        # store the loss \n",
    "        LOSS_Loader.append(criterion(Yhat, Y).tolist())\n",
    "        \n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            # make a prediction\n",
    "            yhat = forward(x)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = criterion(yhat, y)\n",
    "            \n",
    "            # Section for plotting\n",
    "            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
    "            \n",
    "            # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updata parameters slope\n",
    "            w.data = w.data - lr * w.grad.data\n",
    "            b.data = b.data - lr* b.grad.data\n",
    "            \n",
    "            # Clear gradients \n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "            \n",
    "        #plot surface and data space after each epoch    \n",
    "        get_surface.plot_ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 iterations\n",
    "\n",
    "train_model_DataLoader(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the LOSS_BGD and LOSS_Loader\n",
    "\n",
    "plt.plot(LOSS_BGD,label=\"Batch Gradient Descent\")\n",
    "plt.plot(LOSS_Loader,label=\"Stochastic Gradient Descent with DataLoader\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Cost/ total loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
